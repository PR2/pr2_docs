#!/usr/bin/env python
#
# Notes:
#
#  run with: python pr2manual.py pr2.toc.txt
from __future__ import with_statement

import sys
import yaml
import urllib2
import re
import os
import urlparse
import time
import copy

last_tag =''

# New-style BSD
# http://www.crummy.com/software/BeautifulSoup/
from BeautifulSoup import BeautifulSoup, Tag , NavigableString

def _load_yaml(filename):
    with open(filename, 'r') as f:
        s = f.read()
    # only support one document
    return [d for d in yaml.load_all(s)][0]
    
def _cleanup_wiki_parsetree(soup):
    # filter out menus, span markup, etc...
    # - span markup
    span_anchors = soup.findAll('span', **{'class': "anchor"})
    [a.extract() for a in span_anchors]
    # - ToC
    res = soup.findAll('div', **{'class': 'table-of-contents'})
    [r.extract() for r in res]
    # - Include menus
    res = soup.findAll('div', id=re.compile('^Include_Menus'))
    [r.extract() for r in res]
    # - Search results
    res = soup.findAll('div', **{'class': 'searchresults'})
    [r.extract() for r in res]

    # remove manifest header
    res = soup.findAll('div', **{'class': 'manifest'})
    [r.extract() for r in res]

    # remove templated links to autogenerated documentation and Troubleshooting
    res = soup.findAll('a', text="auto-generated code documentation")
    [r.extract() for r in res]
    res = soup.findAll('a', text="Troubleshooting")
    [r.extract() for r in res]
    
    
    # clear out package review template stuff
    res = soup.findAll(text="Package review meeting notes")
    for r in res:
        r.parent.parent.extract()
    res = soup.findAll(text="Create new package review")
    for r in res:
        t = r.parent.parent
        sibs = [t.nextSibling, t.nextSibling.nextSibling]
        t.extract()
        for s in sibs:
            s.extract()
    
    # - cleanout empty paragraphs
    res = soup.findAll('p')
    for r in res:
        text = ''.join([str(c) for c in r.contents])
        if not text.strip():
            r.extract()
    
def _pushdown_headers(soup, level):
    #all headers need to be recontexted to current level
    for hl in reversed(xrange(1, 6)):
        res = soup.findAll('h%s'%hl)
        for r in res:
            r.name = 'h%s'%(hl+level)
            r.attrs = []

def _read_single_page(uri, use_cache):
    # pulled this out from fetch_pages to isolate the caching logic
    parsed = urlparse.urlparse(uri)

    # ensure the cache directory exists
    cache_root = 'pr2manual_cache'
    cache_fn = os.path.join(cache_root, (parsed.netloc + parsed.path).replace('/','___'))
    if use_cache and not os.path.exists(cache_root):
        os.mkdir(cache_root)
    # if requested, see if we have a cache hit
    if use_cache and parsed.scheme == 'http':
        print cache_fn
        if os.path.exists(cache_fn) and (time.time() - os.path.getmtime(cache_fn) < 60*60):
            with open(cache_fn, 'r') as f:
                return f.read()
    # cache miss or cache not requested. go grab it from the interwebs
    try:
        p = urllib2.urlopen(uri)
        data = p.read()
        if use_cache:
            with open(cache_fn, 'w') as f:
                f.write(data)
        return data
    except urllib2.HTTPError:
        print "Failed to fetch", uri
        sys.exit(-1)

def _fetch_pages(d, level=1, use_cache=False, flatten=True):
    for entry in d:
        assert isinstance(entry, dict)
        for k, v in entry.iteritems():
            try:
                if type(v) == list:
                    _fetch_pages(v, level+1, use_cache)
                    continue
                if v == None:
                  entry[k] = "Empty section"
                  continue
                if v.strip().startswith('http:'):
                    p_text = _read_single_page(v, use_cache)
                    soup = BeautifulSoup(p_text)
    
                    # remove markup we don't want
                    _cleanup_wiki_parsetree(soup)
    
                    # push-down
                    _pushdown_headers(soup, level)
                    
                    # scan to the content div of wiki
                    contents = soup.findAll('div', id="content")
                    
                    content = contents[0]
                    entry[k] = { }
                    entry[k]['text'] = str(content)
                    entry[k]['tree'] = content
                else:
                    # this is just text, leave it be
                    text = entry[k]
                    entry[k] = { }
                    entry[k]['text'] = text
                    entry[k]['tree'] = text 
            except:
                print "Error processing page %s: %s"%(k, v)
                import traceback
                traceback.print_exc()
                sys.exit(-1)
        
def _anchor_key(k):
    return k.replace(' ', '_')

def _sub_convert_to_html(entry, level=1):
    assert len(entry.keys()) == 1
    k = entry.keys()[0]
    safe_k = _anchor_key(k)

    # yield anchor and header
    yield '<a name="%s"></a>'%safe_k
    yield '<h%s>%s</h%s>\n'%(level, k, level)
    
    val = entry[k]
    if type(val) == list:
        # yield each sub entry
        for v in val:
            for l in _sub_convert_to_html(v, level+1):
                yield l
    else:
        text = val['text']
        # yield the text
        assert isinstance(text, basestring)
        yield text+'\n'

def _latex_division(title, level):
    if level == 2:
        return "\chapter{%s}\n"%title
    elif level == 3:
        return  "\section{%s}\n"%title
    elif level == 4:
        return "\subsection{%s}\n"%title
    elif level == 5:
        return "\subsubsection{%s}\n"%title
    else:
        return "\\textbf{%s}\n"%title

def _descend_into_tag(tag):
  global last_tag
  #print type(tag).__name__
  if type(tag).__name__ == 'str' or type(tag).__name__ == 'NavigableString':
    #print "tag type %s found string %s last_tag: %s" % (type(tag).__name__ , tag, last_tag)
    if last_tag == 'pre':
        tag = tag.replace('&gt;','>')
        tag = tag.replace('&lt;','<')
        yield tag 
        return
    tag = tag.replace('&gt;','>')
    tag = tag.replace('&lt;','<')
    tag = tag.replace('"','\"')
    tag = tag.replace('<','')
    tag = tag.replace('>','')
    tag = tag.replace('_','\_')
#    tag = tag.replace('/','\/')
    tag = tag.replace('%','\%')
    tag = tag.replace('&','\&')
    tag = tag.replace('$','\$')
    tag = tag.replace('#','\#')
    yield tag
    return
  #print "tag names", tag.name
  strip_leading_numbers = False
  if tag.name == 'b' or tag.name == 'em' or tag.name == 'strong':
    last_tag ='b'
    yield "\\textbf{ "
  elif tag.name == 'p':
    last_tag ='p'
    yield "\n"
  elif tag.name == 'a':
    last_tag ='a'
    pass
  elif tag.name == 'h5' or tag.name =='h2':
    last_tag ='h5'
    strip_leading_numbers = True
    #print tag.name, tag.contents
    yield "\\section{"
  elif tag.name == 'h6' or tag.name == 'h7' or tag.name == 'h8' or tag.name == 'h9' or tag.name == 'h10':
    last_tag ='h6'
    strip_leading_numbers = True
    #print tag.name, tag.contents
    yield "\\subsection{"
  elif tag.name == 'img':
    last_tag ='img'
    pass
  elif tag.name == 'ul':
    last_tag ='ul'
    yield "\n\\begin{itemize}\n"
    #yield _latex_division(tag.contents[0], 3)
  elif tag.name == 'ol':
    last_tag ='ol'
    yield "\n\\begin{enumerate}\n"
  elif tag.name == 'tt':
    last_tag ='tt'
    yield "\\texttt{"  
  elif tag.name == 'li':
    last_tag ='li'
    #print tag.name
    #print "li parent: %s tag %s\n\n" % (tag.parent.name, tag)
    yield "\item "
  elif tag.name == 'pre':
    last_tag ='pre'
    yield "\n\\begin{code}\n"
  elif tag.name == 'span' and tag.has_key('class'):
    last_tag ='span'
    if tag['class'] == 'macro-nav-menu':
      return # ignore these guys
  elif tag.name =='div':
    last_tag ='div'
    pass
  elif tag.name == 'table':
    last_tag ='table'
    pass
  elif tag.name == 'tbody':
    last_tag ='table'
    pass
  elif tag.name == 'tr':
    last_tag ='tr'
    pass
  elif tag.name == 'td':
    last_tag ='td'
    pass
  else:
    last_tag ='unhandled'
    #print "unhandled tag: [%s]"%tag.name
    return

  if tag.name != 'img':
    for c in tag.contents:
      for s in _descend_into_tag(c):
        #print tag.name
        if strip_leading_numbers:
          wds = s.strip().split()
          yield ' '.join(wds[1:])
        else:
          if tag.name == 'td' and 'Figure' in s:
            pass
          else:
            yield s
  else:
    if 'begin{tabular}' in tag['alt']:
      yield '\\vspace{8mm}\n'+tag['alt']+'\\vspace{8mm}\n'
    else:
      width="150"
      if tag.has_key("width"):
        if tag["width"]=="90%":
          width="300"
        if tag["width"]=="70%":
          width="250"
      target = tag['src'].split("target=")
      yield "\\begin{figure}[h!]\n\centering\n\includegraphics[width=%spx]{images/%s}\n\caption{%s}\n\label{fig:%s}\n\\end{figure}\n"%(width,target[1],tag['alt'],target[1])

  if tag.name =='b' or tag.name == 'em' or tag.name == 'strong':
    yield "} "
  elif tag.name == 'h3':
    yield "}"
  elif tag.name == 'h5' or tag.name== 'h2' or tag.name == 'h6' or tag.name == 'h7' or tag.name == 'h8' or tag.name == 'h9' or tag.name == 'h10':
    yield "} "
  elif tag.name == 'p':
    yield "\n"
  elif tag.name == 'a':
    pass
  elif tag.name == 'img':
    pass
  elif tag.name == 'ul':
    yield "\n\\end{itemize}\n"
  elif tag.name == 'ol':
    yield "\n\\end{enumerate}\n"
  elif tag.name == 'li':
    pass
  elif tag.name == 'tt':
    yield "}" #\\end{code}\n"
  elif tag.name == 'pre':
    yield "\n\\end{code}\n"
 


    #yield str(tag.contents[0])
  #print tag.name
  #print str(tag.contents[0])
  #print "\n\n"
    
def _sub_convert_to_latex(entry, level=1):
    assert len(entry.keys()) == 1
    k = entry.keys()[0]
    safe_k = k.replace('_',' ')
    yield _latex_division(safe_k, level)

    val = entry[k]
    if type(val) == list:
        for v in val:
            for l in _sub_convert_to_latex(v, level+1):
                yield l
    else:
        #soup = BeautifulSoup(val['tree'])
        for tag in val['tree']: #.findAll():
          for s in _descend_into_tag(tag):
            yield s
#          yield _descend_into_tag(tag)
        #          print tag.name



        #print soup
        #sys.exit(1)
        #assert isinstance(val, basestring)
        #val = val.replace('<','')
        #val = val.replace('>','')
        #val = val.replace('_','\_')
        #val = val.replace('&','\&')
        #val = val.replace('$','\$')
        #val = val.replace('#','\#')
        #yield val+'\n'

def _sub_convert_toc_to_html(doc, depth, max_depth):
    if depth > max_depth:
        return
    for entry in doc:
        assert isinstance(entry, dict)
        for k, v in entry.iteritems():
            #print k
            yield '<li><a href="#%s">%s</a>'%(_anchor_key(k), k)
            if type(v) == list:
                yield '<ul>'
                for l in _sub_convert_toc_to_html(v, depth+1, max_depth):
                    yield l
                yield '</ul>'
            yield '</li>'
    
def _convert_to_html(doc):
    with open('manual.html', 'w') as f:
        f.write('<html><head><title>PR2 Users Manual</title></head>\n')
        f.write('<body>\n')
        f.write('<h1>PR2 Users Manual</h1>\n')
        f.write('<h2>Table of Contents</h2>\n')
        for l in _sub_convert_toc_to_html(doc, depth=1, max_depth=3):
            f.write(l+'\n')

        for entry in doc:
            for l in _sub_convert_to_html(entry, level=2):
                f.write(l+'\n')
        f.write('</body></html>')

def _convert_to_latex(doc):
    with open('manual.tex', 'w') as f:
        with open('pr2_title_page.tex','r') as header_file:
            f.write(header_file.read())
        for entry in doc:
            for l in _sub_convert_to_latex(entry, level=2):
                f.write(l)
        with open('manual_latex_footer.tex','r') as footer_file:
            f.write(footer_file.read())
    print "running latex"
    os.system('pdflatex manual.tex && pdflatex manual.tex')
    print "done"
    
def load_manual(filename, cache):
    d = _load_yaml(filename)
    _fetch_pages(d, level=2, use_cache=cache)
    _convert_to_html(d)
    _convert_to_latex(d)

def rosmanual_main():
    import optparse
    parser = optparse.OptionParser(usage="usage: rosmanual <manual-yaml>")
    parser.add_option("-c", "--cache", action="store_true", dest="use_cache",
                      default=False,
                      help="cache wiki pages locally for an hour")
    options, args = parser.parse_args()
    if len(args) != 1:
        parser.error("you must specify one and only one filename")
    load_manual(args[0], options.use_cache)

if __name__ == '__main__':
    rosmanual_main()
